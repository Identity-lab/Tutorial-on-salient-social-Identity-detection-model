{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** If unfamiliar with Jupyter Notebooks, instructions for installing and running can be found here: http://jupyter.org/install. Before installing Jupyter Notebook, make sure that Python is installed (our code is with Python3) in your system. We recommend installing Python and Jupyter using the conda package manager.***\n",
    "\n",
    "\n",
    "# Data Prepration\n",
    "\n",
    "We need to take a few steps to convert the raw data (forum text data) to the one (LIWC feature vectos) that we can use for our salient social identity detection model:\n",
    "\n",
    "1- Cleaning\n",
    "\n",
    "2- Importing to database\n",
    "\n",
    "3- Output processed text for input to LIWC\n",
    "\n",
    "4- Processing for LIWC features\n",
    "\n",
    "5- Importing LIWC vectors back into db\n",
    "\n",
    "6- Sampling LIWC vectors to csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning \n",
    "\n",
    "These are the steps I used to preprocesses raw csv files to remove copy artifacts, and make good for import into the database. These steps are particularly relevant to the Mumsnet dataset and Reddit dataset, but they may be helpful for other data too.\n",
    "\n",
    "### Removing new lines\n",
    "by removing the new lines we convert each message into a single line, which is compatible with LIWC software. First I made a copy of the csv files, for example ParentMessages.csv (copies have $\\_$ clean added to file stem), then I run the following sed commands to clean them up: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping deleted posts\n",
    "\n",
    "Sometimes a post is deleted by the moderator of forum or by user herself. \n",
    "\n",
    "\n",
    "In case of mumsnet, these posts could be detected by matching with patterns like \"Message deleted\", \n",
    "\"Message deleted by Mumsnet\", \"Message withdrawn\", \"Message withdrawn at poster's request\". \n",
    "\n",
    "\n",
    "For reddit data, there are patterns like \"[deleted]\", \"[removed]\", \"[deleted by user]\". \n",
    "We also drop the posts which belong to users who are no longer in the platform, by matching with patterns like\n",
    "\"[deleted]\", and \"[removed]\". \n",
    "\n",
    "\n",
    "### Dropping bots posts\n",
    "There are some posts generated with bots and contains patterns like  \"I'm a bot...\", . This is particularly relevant to reddit data but it may be helpful for other data too.\n",
    "\n",
    "\n",
    "We run regular expression to detect and drop these posts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the csv files\n",
    "csv_path = './preprocessing_test/raw_data/'\n",
    "file_name = 'comments_test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# reading reddit file\n",
    "test = pd.read_csv(csv_path+file_name+'.csv', encoding='utf-8')\n",
    "\n",
    "test = test[['author', 'id', 'body']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#regelar expressions to find the matched patterns\n",
    "mumsnet_regex_1 = \"message deleted\"\n",
    "mumsnet_regex_2 = \"message deleted by mumsnet.\"\n",
    "mumsnet_regex_3 = \"message deleted by mumsnet for breaking our talk guidelines.\"\n",
    "mumsnet_regex_4 = \"message withdrawn at poster's request.\"\n",
    "mumsnet_regex_5 = \"message withdrawn by mumsnet.\"\n",
    "\n",
    "\n",
    "reddit_regex_1 = '\\[deleted\\]'\n",
    "reddit_regex_2 = '\\[removed\\]'\n",
    "reddit_regex_3 = '(?i)I\\'m a bot'\n",
    "reddit_regex_4 = '(?i)I ?\\^?(\\'?m|am) \\^?\\^?a \\^?\\^?bot'\n",
    "reddit_regex_5 = '(?i)this bot wants to find'\n",
    "reddit_regex_6 = '\\[deleted by user\\]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def regex_match(text, regex):\n",
    "    if not (re.search(regex, text) is None):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "    \n",
    "def reddit_cm_cleaning(text):\n",
    "    proctext = str(text).lower()\n",
    "    if (regex_match(proctext, reddit_regex_1) or regex_match(proctext, reddit_regex_2) \n",
    "        or regex_match(proctext, reddit_regex_3) or regex_match(proctext, reddit_regex_4)\n",
    "        or regex_match(proctext, reddit_regex_5) or regex_match(proctext, reddit_regex_6)):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "    \n",
    "def reddit_author_cleaning(text):\n",
    "    proctext = str(text).lower()\n",
    "    if (regex_match(proctext, reddit_regex_1) or regex_match(proctext, reddit_regex_2)):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "    \n",
    "def mumsnet_cm_cleaning(text):\n",
    "    proctext = str(text).lower().strip()\n",
    "    if (regex_match(proctext, mumsnet_regex_1) or regex_match(proctext, mumsnet_regex_2) \n",
    "        or regex_match(proctext, mumsnet_regex_3) or regex_match(proctext, mumsnet_regex_4)\n",
    "        or regex_match(proctext, mumsnet_regex_5)):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reddit_cleaned_mgs(df):\n",
    "    df['cleaned'] = df['body'].apply(reddit_cm_cleaning)\n",
    "    idlist = df.loc[df['cleaned'] == False]['id']\n",
    "    \n",
    "    f_df = df.loc[df['id'].isin(idlist)]\n",
    "    \n",
    "    f_df['cleaned'] = f_df['author'].apply(reddit_author_cleaning)\n",
    "    idlist = f_df.loc[f_df['cleaned'] == False]['id']\n",
    "    \n",
    "    f_df = f_df.loc[df['id'].isin(idlist)]\n",
    "    \n",
    "    return f_df[['author', 'id', 'body']]\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mumsnet_cleaned_mgs(df):\n",
    "    df['cleaned'] = df['body'].apply(mumsnet_cm_cleaning)\n",
    "    idlist = df.loc[df['cleaned'] == False]['id']\n",
    "    \n",
    "    f_df = df.loc[df['id'].isin(idlist)]\n",
    "    \n",
    "    return f_df[['author', 'id', 'body']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = reddit_cleaned_mgs(test)\n",
    "df = mumsnet_cleaned_mgs(df)\n",
    "\n",
    "df.to_csv(csv_path + file_name + '_cleaned.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "There are a few other cleaning steps which are done in the export from db to LIWC input functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing to the database\n",
    "\n",
    "Next step is importing the cleaned csv files into the database. During this step, we create a unique user_id for each user_name in our database.\n",
    "\n",
    "Having our data in format of a table gives us the following advantagous:\n",
    "\n",
    "- visualization (having access to different parts our data)\n",
    "- managable (updating our dataset)\n",
    "- searchable (searching for a specific user or post by mysql commands)\n",
    "\n",
    "for this, we first need to install sqlite3. SQLite is a relational database management system that implements a small, fast, self-contained, high-reliability, full-featured, SQL database engine. In contrast to many other database management systems, SQLite is not a clientâ€“server database engine. Rather, it is embedded into the end program.  SQLite is the most used database engine in the world, and it is built into all mobile phones and most computers and comes bundled inside countless other applications that people use every day.  instructions for installing and running can be found here: https://mislav.net/rails/install-sqlite3/  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sqlite3\n",
    "\n",
    "\n",
    "def get_path(fpath=None, fdir=None, fstem=None, fext=None):\n",
    "    \n",
    "    if fpath is None:\n",
    "        fpath = fdir + os.sep + fstem + '.' + fext\n",
    "    fpath = os.sep.join(fpath.split('/'))\n",
    "    \n",
    "    return fpath\n",
    "\n",
    "\n",
    "def connect_to_db(dbdir, dbstem, dbpath=None):\n",
    "    print(\"Connecting to db with dbdir = %s\" % dbdir)\n",
    "    print(\"Connecting to db with dbstem = %s\" % dbstem)\n",
    "    \n",
    "    dbpath = get_path(\n",
    "        fpath=dbpath, fdir=dbdir, fstem=dbstem, fext='db')\n",
    "    conn = sqlite3.connect(dbpath)\n",
    "    \n",
    "    return conn\n",
    "\n",
    "\n",
    "def create_tables(conn, cursor):\n",
    "\n",
    "    query = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS comments(\n",
    "            comment_id varchar(255) PRIMARY KEY,\n",
    "            author_id varchar(255), body TEXT)\n",
    "        \"\"\"\n",
    "\n",
    "    cursor.execute(query)\n",
    "\n",
    "    query = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS authors(\n",
    "            author_id INTEGER PRIMARY KEY, author_name varchar(255),\n",
    "            FOREIGN KEY(author_id) REFERENCES comments(author_id))\n",
    "        \"\"\"\n",
    "\n",
    "    cursor.execute(query)\n",
    "\n",
    "    conn.commit()\n",
    "    return conn, cursor\n",
    "\n",
    "\n",
    "\n",
    "def create_db(dbdir, dbstem):\n",
    "    conn = connect_to_db(dbdir=dbdir, dbstem=dbstem)\n",
    "    cursor = conn.cursor()\n",
    "    create_tables(conn, cursor)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to db with dbdir = ./preprocessing_test/sqlit_database\n",
      "Connecting to db with dbstem = comment_test\n"
     ]
    }
   ],
   "source": [
    "DBDIR = './preprocessing_test/sqlit_database'\n",
    "DBSTEM = 'comment_test'\n",
    "SUBDIR = 'LIWC'\n",
    "\n",
    "create_db(DBDIR, DBSTEM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def write_comments_to_db(conn, cursor, file_name):\n",
    "    with open(file_name, 'r') as handler:\n",
    "        reader = csv.reader(handler, delimiter=',')\n",
    "        # we ignore the first row\n",
    "        firstrow = next(reader)\n",
    "        print(firstrow)\n",
    "        \n",
    "        cursor.execute('BEGIN TRANSACTION')\n",
    "        author_ids = {}\n",
    "        count = 1\n",
    "        for i, row in enumerate(reader):\n",
    "            if len(row) == 0:\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                author_name = str(row[0])                  \n",
    "            except IndexError:\n",
    "                raise\n",
    "                \n",
    "            try:\n",
    "                comment_id = str(row[1])\n",
    "            except IndexError:\n",
    "                raise   \n",
    "                \n",
    "            try:\n",
    "                body = row[2]\n",
    "            except IndexError:\n",
    "                raise\n",
    "                \n",
    "\n",
    "            if author_name in author_ids:\n",
    "                author_id = author_ids[author_name]\n",
    "            else:\n",
    "                author_id = count\n",
    "                author_ids[author_name] = author_id\n",
    "                count += 1\n",
    "\n",
    "\n",
    "            query_1 = \"\"\"\n",
    "                INSERT INTO comments(author_id, comment_id, body) VALUES(?,?,?)\n",
    "                \"\"\"\n",
    "\n",
    "            query_2 = \"\"\"\n",
    "                INSERT INTO authors(author_name, author_id) VALUES(?,?)\n",
    "                \"\"\"\n",
    "\n",
    "            try:\n",
    "                cursor.execute(query_1, (author_id, comment_id, body))\n",
    "            except sqlite3.IntegrityError:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                cursor.execute(query_2, (author_name, author_id))\n",
    "            except sqlite3.IntegrityError:\n",
    "                pass\n",
    "\n",
    "        cursor.execute('COMMIT')\n",
    "\n",
    "\n",
    "\n",
    "#writing comments into db\n",
    "def prepare_comment_table(dbdir, dbstem, file_name):\n",
    "    conn = connect_to_db(dbdir=dbdir, dbstem=dbstem)\n",
    "    cursor = conn.cursor()\n",
    "    write_comments_to_db(conn, cursor, file_name)   \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to db with dbdir = ./preprocessing_test/sqlit_database\n",
      "Connecting to db with dbstem = comment_test\n",
      "['author', 'id', 'body']\n"
     ]
    }
   ],
   "source": [
    "prepare_comment_table(DBDIR, DBSTEM, csv_path+file_name+'_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to db with dbdir = ./preprocessing_test/sqlit_database\n",
      "Connecting to db with dbstem = comment_test\n",
      "0 300000 Writing to ./preprocessing_test/sqlit_database/LIWC/minimal_seg0.txt...\n",
      "File lengths are consistent, with 7 items\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import sys\n",
    "\n",
    "\n",
    "def make_directory(outdir):\n",
    "    try:\n",
    "        os.makedirs(outdir)\n",
    "    except OSError as exc:  \n",
    "        if exc.errno == os.errno.EEXIST and os.path.isdir(outdir):\n",
    "            pass\n",
    "        else:\n",
    "            raise\n",
    "            \n",
    "            \n",
    "\n",
    "def cursor_execute_Results(cursor, selectquery):\n",
    "    cursor.execute(selectquery)\n",
    "\n",
    "    user_results = [\n",
    "        (str(post_id), str(body))\n",
    "        for post_id, body in cursor.fetchall()]\n",
    "    post_ids, texts = zip(*user_results)\n",
    "\n",
    "    return post_ids, texts\n",
    "\n",
    "\n",
    "\n",
    "def cleaning_for_LIWC(text, lowercase=True):\n",
    "    WHITESPACEREGEX = r'[ \\t\\n\\r\\f\\v]+'\n",
    "    NONPUNCREGEX = r'[a-zA-Z0-9_\\s]'\n",
    "    \n",
    "    if lowercase:\n",
    "        proctext = text.lower()\n",
    "    else:\n",
    "        proctext = text\n",
    "            \n",
    "    proctext = proctext.strip()\n",
    "    \n",
    "    # now replace newlines with space \n",
    "    proctext.replace('\\n',' ')\n",
    "\n",
    "    \n",
    "    # remove all duplicate whitespace\n",
    "    proctext = re.sub(WHITESPACEREGEX, ' ', proctext)\n",
    "    \n",
    "    # if text is only punctuation, remove it\n",
    "    if re.search(NONPUNCREGEX, proctext) is None:\n",
    "        proctext = ''\n",
    "        \n",
    "    return proctext\n",
    "\n",
    "\n",
    "\n",
    "def write_single_file(txtfpath, idfpath, post_ids, texts, \n",
    "                      lowercase=None, encode_as=None, parasep=None,\n",
    "                      with_dummies=False, dummytext='xxxdummyxxx'):\n",
    "\n",
    "    with open(txtfpath, 'w') as of_handler, open(idfpath, 'w') as idf_handler:\n",
    "        # iterate through and write each to file\n",
    "        # cursor.execute(selectquery)\n",
    "        count = 0\n",
    "        for i, (post_id, text) in enumerate(zip(post_ids, texts)):\n",
    "\n",
    "            # if the text does not contain any words then skip it\n",
    "            if re.search('[a-zA-Z]', text) is None:\n",
    "                continue\n",
    "\n",
    "            text = cleaning_for_LIWC(text,\n",
    "                                     lowercase=lowercase)\n",
    "\n",
    "            if len(text) == 0:\n",
    "                continue\n",
    "\n",
    "            # insert paragraph separator if line is greater than 0\n",
    "            if count > 0:\n",
    "                of_handler.write(parasep)\n",
    "                \n",
    "            # now encode text (if needed) and write to file\n",
    "            if not encode_as is None:\n",
    "                # text = text.encode(encode_as) #it's not working with python3\n",
    "                text = str(text)\n",
    "                \n",
    "            # adding a dummy word at the enf of each post\n",
    "            of_handler.write(text + ' ' + dummytext)  # without it, the liwc results was inconsistant with our data\n",
    "            \n",
    "            # if insertion of dummy lines required, do that here. it is optional\n",
    "            if with_dummies:\n",
    "                of_handler.write(parasep)\n",
    "                of_handler.write(dummytext)\n",
    "\n",
    "            idf_handler.write(str(post_id) + '\\n')\n",
    "            count += 1\n",
    "            if (i % 10000) == 0:\n",
    "                # print('post_id = %r' % (post_id,))\n",
    "                sys.stdout.flush()\n",
    "                \n",
    "                \n",
    "def query_clean_and_write_singlefiles(outdir, stem, cursor, selectquery, segmentation,\n",
    "                                      lowercase, encode_as, parasep,\n",
    "                                      with_dummies):\n",
    "    if segmentation:\n",
    "        post_ids, texts = cursor_execute_Results(cursor, selectquery)\n",
    "\n",
    "        \n",
    "        segment_size = 300000\n",
    "        no_segments = int(len(post_ids) / segment_size)\n",
    "        index = 0\n",
    "        seg_no = 0\n",
    "        while seg_no < no_segments:\n",
    "            txtfpath = outdir + os.sep + stem + '_seg{}'.format(seg_no) + '.txt'\n",
    "            idfpath = outdir + os.sep + stem + '_seg{}'.format(seg_no) + '.ids'\n",
    "\n",
    "            post_ids_seg, texts_seg = post_ids[index:(seg_no + 1) * segment_size], texts[\n",
    "                                                                                   index:(seg_no + 1) * segment_size]\n",
    "            print(index, (seg_no + 1) * segment_size, \"Writing to %s...\" % (txtfpath,))\n",
    "\n",
    "            write_single_file(txtfpath, idfpath, post_ids_seg, texts_seg, \n",
    "                              lowercase=lowercase, \n",
    "                              encode_as=encode_as, parasep=parasep, with_dummies=with_dummies)\n",
    "\n",
    "            index = int((seg_no + 1) * segment_size)\n",
    "            seg_no += 1\n",
    "\n",
    "            if not test_numlines_txt_vs_ids(txtfpath, idfpath, with_dummies=with_dummies):\n",
    "                raise ValueError('File lines do not match')\n",
    "\n",
    "        if index < len(post_ids):\n",
    "            txtfpath = outdir + os.sep + stem + '_seg{}'.format(seg_no) + '.txt'\n",
    "            idfpath = outdir + os.sep + stem + '_seg{}'.format(seg_no) + '.ids'\n",
    "\n",
    "            post_ids_seg, texts_seg = post_ids[index:], texts[index:]\n",
    "            print(index, (seg_no + 1) * segment_size, \"Writing to %s...\" % (txtfpath,))\n",
    "            write_single_file(txtfpath, idfpath, post_ids_seg, texts_seg, \n",
    "                              lowercase=lowercase, \n",
    "                              encode_as=encode_as, parasep=parasep, with_dummies=with_dummies)\n",
    "\n",
    "            if not test_numlines_txt_vs_ids(txtfpath, idfpath, with_dummies=with_dummies):\n",
    "                raise ValueError('File lines do not match')\n",
    "\n",
    "    else:\n",
    "        txtfpath = outdir + os.sep + stem + '.txt'\n",
    "        idfpath = outdir + os.sep + stem + '.ids'\n",
    "        print(\"Writing to %s...\" % (txtfpath,))\n",
    "\n",
    "        \n",
    "        post_ids, texts = cursor_execute_Results(cursor, selectquery)\n",
    "\n",
    "\n",
    "        write_single_file(txtfpath, idfpath, post_ids, texts, \n",
    "                          lowercase=lowercase, \n",
    "                          encode_as=encode_as, parasep=parasep, with_dummies=with_dummies)\n",
    "\n",
    "        if not test_numlines_txt_vs_ids(txtfpath, idfpath, with_dummies=with_dummies):\n",
    "            raise ValueError('File lines do not match')\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "def write_to_file(dbdir, dbstem, subdir, encode_as = None, segmentation=False, with_dummies=False):\n",
    "\n",
    "    lowercase = True\n",
    "    if encode_as is None:\n",
    "        encode_as = 'utf-8'\n",
    "        \n",
    "    stem = 'minimal'\n",
    "    parasep = '\\n\\n\\n'\n",
    "\n",
    "    outdir = dbdir + os.sep + subdir\n",
    "    if not os.path.isdir(outdir):\n",
    "        make_directory(outdir)\n",
    "        \n",
    "    conn = connect_to_db(dbdir=dbdir, dbstem=dbstem)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    query = \"\"\"\n",
    "            SELECT comments.comment_id, comments.body\n",
    "            FROM comments\n",
    "            \"\"\"\n",
    "\n",
    "    query_clean_and_write_singlefiles(\n",
    "        outdir, stem, cursor, query,\n",
    "        lowercase=lowercase, segmentation=segmentation,\n",
    "        encode_as=encode_as, parasep=parasep,\n",
    "        with_dummies=with_dummies)\n",
    "    \n",
    "\n",
    "def test_numlines_txt_vs_ids(txtfpath, idfpath, with_dummies=False):\n",
    "    numlines_txt = sum(1 for line in open(txtfpath))\n",
    "    numlines_ids = sum(1 for line in open(idfpath))\n",
    "    if not with_dummies:\n",
    "        lengths_match = ((numlines_ids * 3) - 2) == numlines_txt\n",
    "    else:\n",
    "        lengths_match = ((numlines_ids * 6) - 2) == numlines_txt\n",
    "\n",
    "    if lengths_match:\n",
    "        print('File lengths are consistent, with %d items' % (numlines_ids,))\n",
    "        return True\n",
    "    else:\n",
    "        print('File lengths are inconsistent')\n",
    "        print('numlines: txt=%d, ids=%d' % (numlines_txt, numlines_ids))\n",
    "        return False\n",
    "    \n",
    "#writing comments and posts from database to text file, to feed that to the LIWC\n",
    "write_to_file(dbdir=DBDIR, dbstem=DBSTEM, subdir=SUBDIR, segmentation=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing for LIWC features\n",
    "\n",
    "This requires a copy of the LIWC software, and a copy of LIWC dictionary (we used LIWC_2007 dictionary). Next step is processing single file(s) with LIWC software. Select the categories you want the LIWC software output. Save the results in the same folder as the text and ids files (for example with label LIWC2007.csv)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Importing LIWC vectors back into db\n",
    " To import LIWC's output csv files back into db, you can run the following code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output processed text for input to LIWC\n",
    "\n",
    "The next step is converting posts to the LIWC vectors. For this, we construct large files which can be consumed in a single processing run in the LIWC software. This was required as large numbers of input files (each with a single post) cause the software to choke. However, there were a number of issues with this approach too.\n",
    "First, there are posts that contain no elements that are recognised by the LIWC dictionary (which excludes punctuation, so pure puctuation texts were ignored). For this, we can insert dummy lines between each posts (it is optional), which have a characteristic LIWC vector, so we can step through and ignore these when reading in the output from the LIWC. We also added a dummy word at the end of each post, to make sure of the consistency of the LIWC results and the input files.\n",
    "Second, there is a limitation on the maximum size of the file which can be processed by LIWC software. Therefore, in case of large files, we split a single file into multiple smaller files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def get_path_(fpath=None, fdir=None, fstem=None, fext=None):\n",
    "    if fpath is None:\n",
    "        fpath = fdir + os.sep + fstem + '.' + fext\n",
    "    fpath = os.sep.join(fpath.split('/'))\n",
    "    return fpath\n",
    "\n",
    "\n",
    "def create_liwc_table(conn, cursor, liwc_fields, liwctablename=None):\n",
    "    parent_table = 'comments'\n",
    "    template = ', '.join(['%s FLOAT'] * len(liwc_fields))\n",
    "    liwc_columns_def = template % tuple(liwc_fields)\n",
    "    query = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS %s(\n",
    "            comment_id varchar(255) PRIMARY KEY, %s,\n",
    "            FOREIGN KEY(comment_id) REFERENCES %s(comment_id))\n",
    "        \"\"\" % (liwctablename, liwc_columns_def, parent_table,)\n",
    "    cursor.execute(query)\n",
    "    conn.commit()\n",
    "\n",
    "\n",
    "def get_liwc_table_insert_query(liwc_fields, liwctablename=None):\n",
    "    liwc_fields_str = ', '.join(liwc_fields)\n",
    "    liwc_valuetok_str = ', '.join(['?'] * len(liwc_fields))\n",
    "    insert_query = \"INSERT INTO %s(comment_id, %s) VALUES(?, %s)\" \\\n",
    "                   % (liwctablename, liwc_fields_str, liwc_valuetok_str)\n",
    "    return insert_query\n",
    "\n",
    "\n",
    "\n",
    "def get_next_row(idsreader, datareader):\n",
    "    comment_id = str(next(idsreader)[0])\n",
    "    dat_row = next(datareader)\n",
    "    data = list(map(float, dat_row[2:]))\n",
    "\n",
    "    return comment_id, data\n",
    "\n",
    "\n",
    "\n",
    "def liwc_to_db(dbdir, dbstem, idsfile, datafile, csvdir, liwctablename):\n",
    "    datapath = get_path_(\n",
    "        fdir=csvdir, fstem=datafile, fext='csv')\n",
    "    idspath = get_path_(\n",
    "        fdir=csvdir, fstem=idsfile, fext='ids')\n",
    "\n",
    "    # print (datapath, idspath)\n",
    "    conn = connect_to_db(dbdir=dbdir, dbstem=dbstem, dbpath=None)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    size = 10000\n",
    "    index = 0\n",
    "    values = []\n",
    "    with open(datapath, 'r') as datahandler, open(idspath, 'r') as idshandler:\n",
    "        datareader = csv.reader(datahandler, delimiter=',')\n",
    "        idsreader = csv.reader(idshandler)\n",
    "        # we ignore the first row\n",
    "        firstrow = next(datareader)\n",
    "        \n",
    "        # first two fields are filename and segment\n",
    "        fields = firstrow[2:]\n",
    "\n",
    "        create_liwc_table(conn, cursor, fields, liwctablename=liwctablename)\n",
    "        insert_query = get_liwc_table_insert_query(fields, liwctablename=liwctablename)\n",
    "\n",
    "        comment_id, data = get_next_row(idsreader, datareader)\n",
    "        values.append([comment_id] + data)\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                comment_id, data = get_next_row(idsreader, datareader)\n",
    "            except StopIteration:\n",
    "                break\n",
    "            values.append([comment_id] + data)\n",
    "\n",
    "            if ((index + 1) % size) == 0:\n",
    "                # in batches of <size> we write to liwc table\n",
    "                cursor.executemany(insert_query, values)\n",
    "                values = []\n",
    "                conn.commit()\n",
    "                #print('*')\n",
    "                sys.stdout.flush()\n",
    "            index += 1\n",
    "\n",
    "    ## finally insert remaining values into table\n",
    "    try:\n",
    "        cursor.executemany(insert_query, values)\n",
    "    except sqlite3.IntegrityError:\n",
    "        raise\n",
    "    conn.commit()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to db with dbdir = ./preprocessing_test/sqlit_database\n",
      "Connecting to db with dbstem = comment_test\n"
     ]
    }
   ],
   "source": [
    "IDSFILE = 'minimal_seg0'\n",
    "DATAFILE = 'LIWC_comment_test'\n",
    "liwc_to_db(dbdir=DBDIR, dbstem=DBSTEM, idsfile=IDSFILE, datafile=DATAFILE, csvdir=DBDIR+'/'+SUBDIR,\n",
    "                       liwctablename='liwc_comments')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Sampling LIWC vectors to csv file\n",
    "\n",
    "This is an optional step but is recommended so that the resulting experiments can be verified by rerunning.\n",
    "\n",
    "Here we describe how to sample LIWC vectors from the database. There is the option to sample subsets of LIWC features, but the recommended approach is to sample the total vector and then to read from this file only those vectors you are interested in for your model. There is an option only to sample messages longer than a certain word-count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_cm = 'SELECT comment_id, author_id FROM comments'\n",
    "QUERY_liwc = 'SELECT * FROM liwc_comments'\n",
    "QUERY_all_comments = 'SELECT %s FROM (SELECT * from liwc_comments as L JOIN (select comment_id,' \\\n",
    "                     'author_id from comments) as P on L.comment_id = P.comment_id)'\n",
    "\n",
    "def get_results_from_db(dbdir, dbstem, features=None):\n",
    "    \n",
    "    conn = connect_to_db(dbdir=dbdir, dbstem=dbstem, dbpath=None)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    if features is None:\n",
    "        cursor.execute(QUERY_liwc)\n",
    "        features_liwc = list(map(lambda x: x[0], cursor.description)) \n",
    "        features_all = ', '.join(list(map(lambda x: str(x), ['author_id']+features_liwc)))\n",
    "        features = features_all\n",
    "    else:\n",
    "        features = ', '.join(list(map(lambda x: str(x), ['author_id', 'comment_id']+features)))\n",
    "        \n",
    "    cursor.execute(QUERY_all_comments % features)\n",
    "\n",
    "\n",
    "    #cursor.execute(query)\n",
    "    features = list(map(lambda x: x[0], cursor.description))\n",
    "\n",
    "\n",
    "    liwc_results = [\n",
    "        (int(features[0]), str(features[1]), list(map(lambda x: float(x), features[2:]))) for features in cursor.fetchall()]\n",
    "    user_ids, comment_ids, liwc_features = zip(*liwc_results)\n",
    "\n",
    "    df_liwc = pd.concat([pd.DataFrame(list(user_ids), columns=['user_id']),\n",
    "                         pd.DataFrame(list(comment_ids), columns=['msg_id']),\n",
    "                         pd.DataFrame(list(liwc_features), columns=features[2:])], axis=1)\n",
    "    return df_liwc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to db with dbdir = ./preprocessing_test/sqlit_database\n",
      "Connecting to db with dbstem = comment_test\n"
     ]
    }
   ],
   "source": [
    "output_file = 'output_test'\n",
    "\n",
    "test_df = get_results_from_db(dbdir=DBDIR, dbstem=DBSTEM)\n",
    "test_df.to_csv(DBDIR+'/'+SUBDIR+'/'+output_file+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>msg_id</th>\n",
       "      <th>WC</th>\n",
       "      <th>WPS</th>\n",
       "      <th>Sixltr</th>\n",
       "      <th>Dic</th>\n",
       "      <th>funct</th>\n",
       "      <th>pronoun</th>\n",
       "      <th>ppron</th>\n",
       "      <th>i</th>\n",
       "      <th>...</th>\n",
       "      <th>Comma</th>\n",
       "      <th>Colon</th>\n",
       "      <th>SemiC</th>\n",
       "      <th>QMark</th>\n",
       "      <th>Exclam</th>\n",
       "      <th>Dash</th>\n",
       "      <th>Quote</th>\n",
       "      <th>Apostro</th>\n",
       "      <th>Parenth</th>\n",
       "      <th>OtherP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>26.67</td>\n",
       "      <td>80.00</td>\n",
       "      <td>53.33</td>\n",
       "      <td>6.67</td>\n",
       "      <td>6.67</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>13.33</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>29.0</td>\n",
       "      <td>14.5</td>\n",
       "      <td>6.90</td>\n",
       "      <td>86.21</td>\n",
       "      <td>65.52</td>\n",
       "      <td>10.34</td>\n",
       "      <td>10.34</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.45</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.45</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>24.00</td>\n",
       "      <td>84.00</td>\n",
       "      <td>64.00</td>\n",
       "      <td>20.00</td>\n",
       "      <td>20.00</td>\n",
       "      <td>12.0</td>\n",
       "      <td>...</td>\n",
       "      <td>12.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>30.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>16.67</td>\n",
       "      <td>83.33</td>\n",
       "      <td>60.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.67</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.33</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>26.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>15.38</td>\n",
       "      <td>69.23</td>\n",
       "      <td>53.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.85</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>33.0</td>\n",
       "      <td>16.5</td>\n",
       "      <td>18.18</td>\n",
       "      <td>87.88</td>\n",
       "      <td>57.58</td>\n",
       "      <td>15.15</td>\n",
       "      <td>9.09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>25.0</td>\n",
       "      <td>12.5</td>\n",
       "      <td>20.00</td>\n",
       "      <td>88.00</td>\n",
       "      <td>60.00</td>\n",
       "      <td>20.00</td>\n",
       "      <td>16.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>12.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7 rows Ã— 82 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id msg_id    WC   WPS  Sixltr    Dic  funct  pronoun  ppron     i  \\\n",
       "0        1      1  15.0  15.0   26.67  80.00  53.33     6.67   6.67   0.0   \n",
       "1        1      2  29.0  14.5    6.90  86.21  65.52    10.34  10.34   0.0   \n",
       "2        1      3  25.0  25.0   24.00  84.00  64.00    20.00  20.00  12.0   \n",
       "3        2      4  30.0  15.0   16.67  83.33  60.00    10.00   0.00   0.0   \n",
       "4        2      5  26.0  13.0   15.38  69.23  53.85     0.00   0.00   0.0   \n",
       "5        2      6  33.0  16.5   18.18  87.88  57.58    15.15   9.09   0.0   \n",
       "6        2      7  25.0  12.5   20.00  88.00  60.00    20.00  16.00   0.0   \n",
       "\n",
       "   ...  Comma  Colon  SemiC  QMark  Exclam  Dash  Quote  Apostro  Parenth  \\\n",
       "0  ...  13.33    0.0   0.00    0.0     0.0   0.0    0.0     0.00      0.0   \n",
       "1  ...   3.45    0.0   0.00    0.0     0.0   0.0    0.0     3.45      0.0   \n",
       "2  ...  12.00    0.0   0.00    0.0     0.0   0.0    0.0     0.00      0.0   \n",
       "3  ...   6.67    0.0   3.33    0.0     0.0   0.0    0.0     0.00      0.0   \n",
       "4  ...   3.85    0.0   0.00    0.0     0.0   0.0    0.0     0.00      0.0   \n",
       "5  ...   6.06    0.0   0.00    0.0     0.0   0.0    0.0     0.00      0.0   \n",
       "6  ...  12.00    0.0   0.00    0.0     0.0   0.0    0.0     0.00      0.0   \n",
       "\n",
       "   OtherP  \n",
       "0     0.0  \n",
       "1     0.0  \n",
       "2     0.0  \n",
       "3     0.0  \n",
       "4     0.0  \n",
       "5     0.0  \n",
       "6     0.0  \n",
       "\n",
       "[7 rows x 82 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
