{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the python implementation of the paper \"ASIA: Automated Social Identity Assessment Using Linguistic Style\"\n",
    "\n",
    "# Introduction\n",
    "\n",
    "This tutorial is aimed at readers who have a basic familiarity with Python; if you have not previously used Python, we recommend that you start by taking the course \"Introduction to Data Science in Python\" from coursera (https://www.coursera.org/learn/python-data-analysis). \n",
    "\n",
    "Please see the original paper for the detailed description of the procedure. (Lines starting with \"#\" are comments and will not be executed by Python.)\n",
    "\n",
    "In order to run the code, first you need to\n",
    "\n",
    "- download the dataset\n",
    "- copy the dataset in a folder named data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specifying some parameters\n",
    "\n",
    "We first need to specify some parameters: \n",
    "\n",
    "1) Size of the training dataset. Here we chose 50000 posts from each forum of parent and femnist (by specifying batch_size = 50000). \n",
    "\n",
    "2) As posts are randomly selected for both training and test, we run our analysis for multiple times, which can be specified by Max_Iter parameter. \n",
    "\n",
    "3) If you want to include only posts which are longer than a specified threshold, you can set WORD_LIMIT = true, and choose the minimum word count by setting MIN_WC parameter. \n",
    "\n",
    "4) DBDIR is pointing to directory that contains dataset, and SAVEDIR is destination for saving the trained model. \n",
    "\n",
    "5) Here we considered all the stylistic features for training our identity detection model, however, any subset of these features can be considered. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DBDIR = './data/'\n",
    "Mumsnet_DB = 'Mumsnet_feminist_parent.csv'\n",
    "Reddit_DB = 'Reddit_feminist_parent.csv'\n",
    "Experimental_DB = 'Experimental_data.csv'\n",
    "\n",
    "SAVEDIR = './save_dir/logr.sav'\n",
    "\n",
    "# all LIWC stylistic features\n",
    "ALL_STYLISTIC_FEATURES = ['WPS', 'i', 'we', 'you', 'shehe', 'they', 'ipron','article', 'auxverb', 'past',\n",
    "                    'present', 'future', 'adverb', 'preps','conj', 'quant', 'number', 'time', 'Sixltr',\n",
    "                    'Period', 'Colon', 'SemiC', 'QMark', 'Dash', 'Quote', 'Apostro', 'Parenth', 'OtherP',\n",
    "                    'negate', 'swear', 'posemo','negemo', 'assent', 'nonfl', 'filler', 'Exclam', 'insight',\n",
    "                    'cause', 'discrep', 'tentat', 'certain', 'inhib', 'incl', 'excl']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some utility functions\n",
    "\n",
    "This section contains functions which prepare data for training and testing.\n",
    "\n",
    "\n",
    "### Reading input files \n",
    "We first read csv files, and preprocess them by removing rows which contain Nan value and dropping short posts if it is specified.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "def preprocessing(df, word_limit, min_WC):\n",
    "    df = df.dropna()\n",
    "    if word_limit:\n",
    "        df = df.loc[df['WC'] >= min_WC]\n",
    "    return df\n",
    "\n",
    "def read_csv(path, word_limit, min_WC):\n",
    "    try:\n",
    "        df = pd.read_csv(path)\n",
    "    except:\n",
    "        print('error in reading file')\n",
    "        raise\n",
    "    df = preprocessing(df, word_limit, min_WC)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Buiding train and test sets\n",
    "\n",
    "In order to build train and test set, we first need to separate users into between and within participants. \n",
    "\n",
    "Within pariticipants are users who are participating in both forums by posting at least once in each forums. Between participants are those who have posted only in one forum (feminist or parent).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separating_users(df):\n",
    "    fem_df = df.loc[df.forum_id == 1]\n",
    "    par_df = df.loc[df.forum_id == 0]\n",
    "    \n",
    "    # participants who are posting in both forums\n",
    "    within_p = set(fem_df.user_id.unique()).intersection(par_df.user_id.unique())\n",
    "    # participants who are posting only in one forum, parent or feminist\n",
    "    between_p = df[~df.user_id.isin(within_p)].user_id.unique()\n",
    "\n",
    "    return between_p, within_p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build the test set by randomly choosing one post per forum for each within participant, if there is no limit on size of the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_testcases(posts_within, no=None):\n",
    "    # randomly selecting one post per forum for each within participant\n",
    "    testDB = posts_within.sample(frac=1)\n",
    "    testDB = testDB.drop_duplicates(subset=['user_id', 'forum_id'])\n",
    "\n",
    "    # if there is no limit on the number of test cases, choose one post per forum\n",
    "    # for each within participant, otherwise, randomly choose no number users from\n",
    "    # within participants\n",
    "    if no is not None:\n",
    "        within_participants = posts_within.user_id.unique()\n",
    "        testUsers = np.random.choice(within_participants, no, replace=False)\n",
    "        testDB = testDB.loc[testDB['user_id'].isin(testUsers)]\n",
    "\n",
    "    return testDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train set is build by randomly choosing posts from between participants, and test set is build by randomly choosing two posts from each within participant one per forum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_between_within_sets(df):\n",
    "    # separating users into between and withing participants\n",
    "    between_participants, within_participants = separating_users(df.copy())\n",
    "\n",
    "    within_set = df.loc[df.user_id.isin(within_participants)]\n",
    "    between_set = df.loc[df.user_id.isin(between_participants)]\n",
    "    \n",
    "    return between_set, within_set\n",
    "\n",
    "\n",
    "def get_train_set(between_set, batch, verbose=True):\n",
    "    between_set = between_set.sample(frac=1)\n",
    "    \n",
    "    if verbose:\n",
    "        print('number of between participants is:{}'.format(len(between_set.user_id.unique())))\n",
    "\n",
    "    # buiding train set by randomly selecting posts from between participants\n",
    "    posts_between_forum1 = between_set[between_set['forum_id'] == 1][:batch]\n",
    "    posts_between_forum0 = between_set[between_set['forum_id'] == 0][:batch]\n",
    "    trainDB = pd.concat([posts_between_forum1, posts_between_forum0])\n",
    "    \n",
    "    return trainDB\n",
    "    \n",
    "\n",
    "def get_test_set(within_set, batch, verbose=True):\n",
    "    within_set = within_set.sample(frac=1)\n",
    "    \n",
    "    if verbose:\n",
    "        print('number of within participants is:{}'.format(len(within_set.user_id.unique())))\n",
    "    \n",
    "    # buidling test set by randomly selecting one posts per from from each within participant\n",
    "    testDB = extract_testcases(within_set)\n",
    "    \n",
    "    return testDB\n",
    "\n",
    "\n",
    "def train_test_prepration(trainDB, testDB, features, standardize=False):\n",
    "    if standardize:\n",
    "        scaler = StandardScaler().fit(trainDB[features])\n",
    "        trainDB[params.features] = scaler.transform(trainDB[features])\n",
    "        testDB[params.features] = scaler.transform(testDB[features])\n",
    "\n",
    "    return trainDB, testDB\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and testing\n",
    "\n",
    "### Choosing a model for training\n",
    "\n",
    "Here we apply Logistic regression for training our model. If grid_search is true, best parameter would be chosen for the model, but it takes longer to train the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "import joblib\n",
    "\n",
    "\n",
    "def Logistic_Regression(grid_search=False):\n",
    "    clf = LogisticRegression(solver='lbfgs', max_iter = 500)\n",
    "\n",
    "    if grid_search:\n",
    "        tuned_parameters = [{'C': [1e-3, 1e-2, 1e-1, 1]}]\n",
    "        clf = GridSearchCV(LogisticRegression(solver='lbfgs'), tuned_parameters, cv=10,\n",
    "                           scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "    return clf\n",
    "\n",
    "\n",
    "def train(X_train, y_train, save_dir, verbose=True, save_model=True):\n",
    "    model = Logistic_Regression()\n",
    "\n",
    "    # training accuracy and AUC\n",
    "    tr_auc = np.mean(cross_val_score(model, X_train, y_train, cv=10, scoring='roc_auc'))\n",
    "    tr_acc = np.mean(cross_val_score(model, X_train, y_train, cv=10, scoring='accuracy'))\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    if save_model:\n",
    "        joblib.dump(model, save_dir)\n",
    "\n",
    "    if verbose:\n",
    "        print('model is trained')\n",
    "\n",
    "    return model, tr_auc, tr_acc\n",
    "\n",
    "\n",
    "def test(clf, X_test, y_test):\n",
    "    y_pred = clf.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    s = clf.decision_function(X_test)\n",
    "    auc = roc_auc_score(y_test, s)\n",
    "\n",
    "    return acc, auc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the identity detection model\n",
    "We then run training and testing for multiple times (here we set that as 20 iterations). In each iteration 100000 posts are randomly selected from the between participant users, which are equally coming from feminist and parent posts. Test set is also built by randomly selecting one post per each within participant from each forum. \n",
    "\n",
    "We report the AUC and accuracy of our identity detection model by averaging over the results from multiple iterations. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def run(trainingDB, testDB, features, max_iter, save_dir, verbose=True):\n",
    "\n",
    "    tr_results= []\n",
    "    tt_results = []\n",
    "    for T in range(max_iter):\n",
    "        print('round:{}'.format(T + 1))\n",
    "\n",
    "        #trainingDB, testDB = split_train_test(df.copy(deep=True), batch_size, word_limit, word_count, features)\n",
    "        X_train, y_train = trainingDB[features], trainingDB['forum_id']\n",
    "        X_test, y_test = testDB[features], testDB['forum_id']\n",
    "\n",
    "        model, tr_auc, tr_acc = train(X_train, y_train, save_dir)\n",
    "        if verbose:\n",
    "            print('training accuracy:{}'.format(tr_acc), 'training AUC :{}'.format(tr_auc))\n",
    "\n",
    "        tt_acc, tt_auc = test(model, X_test, y_test)\n",
    "\n",
    "        if verbose:\n",
    "            print('test accuracy:{}'.format(tt_acc), 'test AUC :{}'.format(tt_auc))\n",
    "\n",
    "        tr_results.append({'acc': tr_acc, 'auc': tr_auc})\n",
    "        tt_results.append({'acc': tt_acc, 'auc': tt_auc})\n",
    "\n",
    "    return tr_results, tt_results\n",
    "\n",
    "\n",
    "def main(verbose=True):\n",
    "    batch_size = 50000\n",
    "    max_iter = 1\n",
    "    word_limit = True\n",
    "    min_WC = 25\n",
    "    features = ALL_STYLISTIC_FEATURES\n",
    "    standardize = False\n",
    "    \n",
    "    dbdir = DBDIR\n",
    "    save_dir = SAVEDIR\n",
    "    \n",
    "    \n",
    "    # choose the dataset you want to test the trained model on\n",
    "    testing_on = 'experimental'\n",
    "    \n",
    "\n",
    "    # reading datasets from CSV files\n",
    "    mumsnet_df = read_csv(dbdir+Mumsnet_DB, word_limit, min_WC)\n",
    "    reddit_df = read_csv(dbdir+Reddit_DB, word_limit, min_WC)\n",
    "    experimental_df = read_csv(dbdir+Experimental_DB, word_limit=False, min_WC=min_WC)\n",
    "    \n",
    "    if verbose:\n",
    "        print('mumsnet_df size:{}'.format(mumsnet_df.shape[0]))\n",
    "        print('reddit_df size:{}'.format(reddit_df.shape[0]))\n",
    "        print('experimental_df size:{}'.format(experimental_df.shape[0]))\n",
    "        \n",
    "    \n",
    "    mumsnet_between_set, mumsnet_within_set = split_between_within_sets(mumsnet_df)\n",
    "    trainDB = get_train_set(mumsnet_between_set, batch_size)\n",
    "    \n",
    "    \n",
    "    if testing_on == 'mumsnet':\n",
    "        testDB = get_test_set(mumsnet_within_set, batch_size)\n",
    "    elif testing_on == 'reddit':\n",
    "        reddit_between_set, reddit_within_set = split_between_within_sets(reddit_df)\n",
    "        testDB = get_test_set(reddit_within_set, batch_size)\n",
    "    elif testing_on == 'experimental':\n",
    "        # choose the topic you want to test on\n",
    "        # there are three topics: healthy mealtimes ('hm'), \n",
    "        # objectification of women ('ow')\n",
    "        # and climate change ('cc')\n",
    "        topic = 'ow'\n",
    "        testDB = experimental_df.loc[experimental_df['topic'] == topic]\n",
    "        testDB = testDB.rename(columns = {'condition' : 'forum_id'})\n",
    "        \n",
    "        \n",
    "    trainDB, testDB = train_test_prepration(trainDB, testDB, features, standardize)\n",
    "    \n",
    "\n",
    "\n",
    "    tr_results, tt_results= run(trainDB, testDB, features, max_iter, save_dir)\n",
    "\n",
    "    print('Accuracy and AUC after {} iterations'.format(max_iter))\n",
    "    print('training_auc:', 'AUC_mean:{}'.format(np.mean([item['auc'] for item in tr_results])),\n",
    "          'AUC_std:{}'.format(np.std([item['auc'] for item in tr_results])))\n",
    "\n",
    "    print('training_acc:', 'acc_mean:{}'.format(np.mean([item['acc'] for item in tr_results])),\n",
    "          'acc_std:{}'.format(np.std([item['acc'] for item in tr_results])))\n",
    "\n",
    "    print('testing_auc:', 'AUC_mean', np.mean([item['auc'] for item in tt_results]), 'AUC_std',\n",
    "          np.std([item['auc'] for item in tt_results]))\n",
    "\n",
    "    print('testing_acc:', 'acc_mean:{}'.format(np.mean([item['acc'] for item in tt_results])),\n",
    "          'acc_std:{}'.format(np.std([item['acc'] for item in tt_results])))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error in reading file\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-78-71b307677215>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(verbose)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;31m# reading datasets from CSV files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mmumsnet_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdbdir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mMumsnet_DB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_limit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_WC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0mreddit_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdbdir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mReddit_DB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_limit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_WC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mexperimental_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdbdir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mExperimental_DB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_limit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_WC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin_WC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-73-dd6ef549f203>\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(path, word_limit, min_WC)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_limit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_WC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'error in reading file'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    683\u001b[0m         )\n\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nrows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1154\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[0;31m# May alter columns / col_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   2057\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2058\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2059\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2060\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2061\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/dtypes/common.py\u001b[0m in \u001b[0;36mis_extension_array_dtype\u001b[0;34m(arr_or_dtype)\u001b[0m\n\u001b[1;32m   1741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1743\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mis_extension_array_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1744\u001b[0m     \"\"\"\n\u001b[1;32m   1745\u001b[0m     \u001b[0mCheck\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0man\u001b[0m \u001b[0mobject\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0mextension\u001b[0m \u001b[0marray\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
