{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In classification, it is typically assumed that the labeled training data comes from the same distribution as that of the test data. However, many real world applications challenge this assumption. These different but related marginal distributions are referred as domains. In this context, the learner must take special care during the learning process to infer models that adapt well to the test data they are deployed on.\n",
    "\n",
    "For example, in study 4 and 5 of our paper, we train the model on mumsnet data and test it on data from reddit and experimental data. Even though both the source and target data lie in the same D-dimensional space (D equals to the number of features or independant variables), they have been drawn according to different marginal distributions. Consequently, rather than working on the original data themselves, the shift between these two source and target domains is learned. This issue is known as domain adaptation(DA). DA typically aims at making use of information coming from both source(train) and target(test) domains during the learning process to adapt automatically.\n",
    "\n",
    "\n",
    "# Subspace Alignment\n",
    "In this tutorial, we apply an unsupervised DA solution proposed in [1], as subspace alignment, to learn a mapping function which aligns the source distribution with the target one (the unsupervised setting means that the training data only need the labelled examples from source data and unlabeled target examples). \n",
    "\n",
    "We go through this process step-by-step, and at end of this tutorial you would be able to apply it on provided datasets by us or datasets of your own. \n",
    "\n",
    "1- First, we transform every source and target data in the form of a D-dimensional z-normalized vector (i.e. of zero mean and unit standard deviation). \n",
    "\n",
    "2- Then, using PCA, for each domain, d eigenvectors is selected which corresponds to the d largest eigenvalues.\n",
    "\n",
    "3- These eigenvectors are used as bases of the source and target subspaces, respectively denoted by X_S and X_T (X_S , X_T ∈ R D×d ). \n",
    "\n",
    "4- Finally, X_S and X_T are used to learn the shift between the two domains.\n",
    "\n",
    "## Tuning the efficient number of eigenvectors (d_max)\n",
    "\n",
    "The unique hyperparameter of the algorithm is the number d of eigenvectors. For this, we first calculate the upper bound of d, as d_max. Afterwards, we consider the subspaces of dimensionality from d = 1 to d_max and select the\n",
    "best d∗ that minimizes the classification error using a 10 fold cross-validation over the labelled source data.\n",
    "More details can be found here [1].\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "[1] Fernando, B., Habrard, A., Sebban, M., & Tuytelaars, T. (2013). Unsupervised visual domain adaptation using subspace alignment. In Proceedings of the IEEE international conference on computer vision (pp. 2960-2967)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "from numpy import linalg as LA\n",
    "import copy\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using Principal Component Analysis (PCA) to identify eigenvectors and eigenvalues\n",
    "def PrincipleC(X, n):\n",
    "    C = np.dot(np.matrix(X).transpose(), np.matrix(X) ) /X.shape[1]\n",
    "    Lambda, U = LA.eig(C)\n",
    "    indices = np.argsort(Lambda)[::-1]\n",
    "    Lambda = [Lambda[i] for i in indices]\n",
    "    U = U[:, indices]\n",
    "    return U[: ,:n], Lambda[:n]\n",
    "\n",
    "#this function calculates the upper bound of d - maximum number of dimensions\n",
    "def get_max_dimension(X_source, X_target, dimensions, verbose=True):\n",
    "    #transform the data in the form of a D-dimensional z-normalized vector\n",
    "    X1 = stats.zscore(X_source)\n",
    "    X2 = stats.zscore(X_target)\n",
    "    \n",
    "    #extract the eigenvectors and eigenvalues\n",
    "    X_s, Lambda_s = PrincipleC(X1.copy(), dimensions)\n",
    "    X_t, Lambda_t = PrincipleC(X2.copy(), dimensions)\n",
    "\n",
    "    lambdas = []\n",
    "    gammas = []\n",
    "    B = 100 #a random positive number\n",
    "    delta = 0.1\n",
    "    n_min = np.minimum(X_source.shape[0], X_target.shape[0])\n",
    "    for i in range(0, dimensions-1):\n",
    "        lmin = np.minimum(Lambda_t[i]-Lambda_t[i+1], Lambda_s[i]-Lambda_s[i+1])\n",
    "        gamma = (1+np.sqrt(math.log(2/delta)/2))*((16*np.power(i+1, 3/2)*B)/(np.sqrt(n_min)*lmin))\n",
    "        lambdas.append({'d': i+1, 'lmin': copy.deepcopy(lmin)})\n",
    "        gammas.append(copy.deepcopy(gamma))\n",
    "\n",
    "    gamma = max(gammas)\n",
    "\n",
    "    d_max = 1\n",
    "    for dic_ in lambdas:\n",
    "        d = dic_['d']\n",
    "        lmin = dic_['lmin']\n",
    "        upper_b = (1+np.sqrt(math.log(2/delta)/2))*((16*np.power(d, 3/2)*B)/(gamma*np.sqrt(n_min)))\n",
    "        if lmin >= upper_b:\n",
    "            d_max = d\n",
    "            \n",
    "    if verbose:\n",
    "        print('\\n upper dimension bound:', d_max)\n",
    "    return d_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "by running the code above, we calculate the upper bound of d, as d_max. In order to calculate the efficient number of dimensions, we run the function get_optimum_dimensions where for dimensionality from d = 1 to d_max, we shiftf source and target data accordignly (function align) and calculate the accuracy of training and testing on source data using 10 fold cross validation. The optimum dimensionality is the one which results in the highest accuracy on source data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# the main function for shifting the source and target distributions\n",
    "def align(X_1, X_2, dim):\n",
    "    X_1 = stats.zscore(X_1)\n",
    "    X_2 = stats.zscore(X_2)\n",
    "\n",
    "    X_s, Lambda_s = PrincipleC(X_1.copy(), dim)\n",
    "    X_t, Lambda_t = PrincipleC(X_2.copy(), dim)\n",
    "\n",
    "    X_a = np.matrix(X_s) * np.matrix(X_s).transpose() * np.matrix(X_t)\n",
    "\n",
    "    S_a = np.matrix(X_1) * np.matrix(X_a)\n",
    "\n",
    "    T_t = np.matrix(X_2) * np.matrix(X_t)\n",
    "\n",
    "    return S_a, T_t\n",
    "\n",
    "# \n",
    "def singletest(model, test_X, test_y, verbose=False):\n",
    "    predict_abs = model.predict(test_X)\n",
    "    probs = model.predict_proba(test_X)\n",
    "    tn, fp, fn, tp = confusion_matrix(test_y, predict_abs).ravel()\n",
    "    fpr, tpr, thresholds = roc_curve(test_y, probs[:, 1], pos_label=1)\n",
    "    \n",
    "    acc_ = (tp + tn) / (tp + tn + fp + fn)\n",
    "    fpr_tpr_auc_ = auc(fpr, tpr)\n",
    "\n",
    "    if verbose:\n",
    "        print('\\n accuracy:', acc_, 'AUC:', fpr_tpr_auc_)\n",
    "        print('tn:', tn, 'fp:', fp, 'fn:', fn, 'tp:', tp)\n",
    "        print('false positive error rate:', fp / (fp + tn))\n",
    "        print('false negative error rate:', fn / (fn + tp))\n",
    "    return acc_\n",
    "\n",
    "\n",
    "# cross validation\n",
    "def test_cross_validation(df, label):\n",
    "    cv = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "    y = df[label].values\n",
    "    X = df.drop(columns=[label]).values\n",
    "    accuracy = []\n",
    "    for train, test in cv.split(X, y):\n",
    "        clf = train(X[train], y[train])\n",
    "        acc_ = singletest(clf, X[test], y[test])\n",
    "        accuracy.append(copy.deepcopy(acc_))\n",
    "    return np.mean(accuracy)\n",
    "\n",
    "\n",
    "# calculating the optimum number of dimensions\n",
    "def get_optimum_dimension(d_max, X_source, y_source, X_target, label='forum_id', verbose=False):\n",
    "    acc_max = 0\n",
    "    d_optimum = 1\n",
    "    for j in range(1, d_max + 1):\n",
    "        X_s, X_t = align(copy.deepcopy(X_source), copy.deepcopy(X_target), j)\n",
    "        train_df = pd.concat([pd.DataFrame(X_s), y_source], axis=1)\n",
    "        acc_ = test_cross_validation(train_df, label=label)\n",
    "        if acc_ >= acc_max:\n",
    "            acc_max = acc_\n",
    "            d_optimum = j\n",
    "    if verbose:\n",
    "        print('\\n maximum accuracy for training and testing on source data:', acc_max)\n",
    "        print('\\n optimum dimensions:', d_optimum)\n",
    "\n",
    "    return d_optimum\n",
    "\n",
    "\n",
    "def get_shared_subspace(X_1, X_2, dim):\n",
    "    scaler_1 = StandardScaler().fit(X_1)\n",
    "    X_1 = scaler_1.transform(X_1)\n",
    "\n",
    "    scaler_2 = StandardScaler().fit(X_2)\n",
    "    X_2 = scaler_2.transform(X_2)\n",
    "\n",
    "    X_s, Lambda_s = PrincipleC(X_1.copy(), dim)\n",
    "    X_t, Lambda_t = PrincipleC(X_2.copy(), dim)\n",
    "\n",
    "    X_a = np.matrix(X_s) * np.matrix(X_s).transpose() * np.matrix(X_t)\n",
    "\n",
    "    return X_a, X_t, scaler_1, scaler_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, there are some general utility functions, for reading the datasets and training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "def preprocessing(df, word_limit, min_WC):\n",
    "    df = df.dropna()\n",
    "    if word_limit:\n",
    "        df = df.loc[df['WC'] >= min_WC]\n",
    "    return df\n",
    "\n",
    "def read_csv(path, word_limit, min_WC):\n",
    "    try:\n",
    "        df = pd.read_csv(path)\n",
    "    except:\n",
    "        print('error in reading file')\n",
    "        raise\n",
    "    df = preprocessing(df, word_limit, min_WC)\n",
    "    return df\n",
    "\n",
    "\n",
    "def separating_users(df):\n",
    "    fem_df = df.loc[df.forum_id == 1]\n",
    "    par_df = df.loc[df.forum_id == 0]\n",
    "    \n",
    "    # participants who are posting in both forums\n",
    "    within_p = set(fem_df.user_id.unique()).intersection(par_df.user_id.unique())\n",
    "    # participants who are posting only in one forum, parent or feminist\n",
    "    between_p = df[~df.user_id.isin(within_p)].user_id.unique()\n",
    "\n",
    "    return between_p, within_p\n",
    "\n",
    "\n",
    "def split_between_within_sets(df):\n",
    "    # separating users into between and withing participants\n",
    "    between_participants, within_participants = separating_users(df.copy())\n",
    "\n",
    "    within_set = df.loc[df.user_id.isin(within_participants)]\n",
    "    between_set = df.loc[df.user_id.isin(between_participants)]\n",
    "    \n",
    "    return between_set, within_set\n",
    "\n",
    "\n",
    "def get_train_set(between_set, batch, verbose=False):\n",
    "    between_set = between_set.sample(frac=1)\n",
    "    \n",
    "    if verbose:\n",
    "        print('number of between participants is:{}'.format(len(between_set.user_id.unique())))\n",
    "\n",
    "    # buiding train set by randomly selecting posts from between participants\n",
    "    posts_between_forum1 = between_set[between_set['forum_id'] == 1][:batch]\n",
    "    posts_between_forum0 = between_set[between_set['forum_id'] == 0][:batch]\n",
    "    trainDB = pd.concat([posts_between_forum1, posts_between_forum0])\n",
    "    \n",
    "    return trainDB\n",
    "\n",
    "\n",
    "def extract_testcases(posts_within, no=None):\n",
    "    # randomly selecting one post per forum for each within participant\n",
    "    testDB = posts_within.sample(frac=1)\n",
    "    testDB = testDB.drop_duplicates(subset=['user_id', 'forum_id'])\n",
    "\n",
    "    # if there is no limit on the number of test cases, choose one post per forum\n",
    "    # for each within participant, otherwise, randomly choose no number users from\n",
    "    # within participants\n",
    "    if no is not None:\n",
    "        within_participants = posts_within.user_id.unique()\n",
    "        testUsers = np.random.choice(within_participants, no, replace=False)\n",
    "        testDB = testDB.loc[testDB['user_id'].isin(testUsers)]\n",
    "\n",
    "    return testDB\n",
    "\n",
    "\n",
    "def get_test_set(within_set, batch, verbose=False):\n",
    "    within_set = within_set.sample(frac=1)\n",
    "    \n",
    "    if verbose:\n",
    "        print('number of within participants is:{}'.format(len(within_set.user_id.unique())))\n",
    "    \n",
    "    # buidling test set by randomly selecting one posts per from from each within participant\n",
    "    testDB = extract_testcases(within_set)\n",
    "    \n",
    "    return testDB\n",
    "\n",
    "\n",
    "def Logistic_Regression(grid_search=False):\n",
    "    clf = LogisticRegression(solver='lbfgs', max_iter = 2000)\n",
    "\n",
    "    if grid_search:\n",
    "        tuned_parameters = [{'C': [1e-3, 1e-2, 1e-1, 1]}]\n",
    "        clf = GridSearchCV(LogisticRegression(solver='lbfgs'), tuned_parameters, cv=10,\n",
    "                           scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "    return clf\n",
    "\n",
    "\n",
    "def train(X_train, y_train, verbose=True):\n",
    "    model = Logistic_Regression()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "    if verbose:\n",
    "        print('model is trained')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we prepare the datasets for applying the DA solution. The source data is the mumsnet dataset (Mumsnet_feminist_parent.csv) and the target data is the reddit dataset(Reddit_feminist_parent.csv) or experimental dataset (Experimental_data_within.csv, Experimental_data_between.csv).\n",
    "\n",
    "First:\n",
    "\n",
    "- download the datasets : Mumsnet_feminist_parent.csv, Reddit_feminist_parent.csv, Experimental_data_within.csv, Experimental_data_between.csv\n",
    "- copy the datasets in a folder named data\n",
    "\n",
    "\n",
    "DBDIR is pointing to directory that contains dataset (data folder).\n",
    "\n",
    "Here we considered all the stylistic LIWC features for training our identity detection model, however, any subset of these features can be considered. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "DBDIR = './data/'\n",
    "Mumsnet_DB = 'Mumsnet_feminist_parent.csv'\n",
    "Reddit_DB = 'Reddit_feminist_parent.csv'\n",
    "Exp_DB_within = 'Experimental_data_within.csv'\n",
    "Exp_DB_between = 'Experimental_data_between.csv'\n",
    "\n",
    "# all LIWC stylistic features\n",
    "ALL_STYLISTIC_FEATURES = ['WPS', 'i', 'we', 'you', 'shehe', 'they', 'ipron','article', 'auxverb', 'past',\n",
    "                    'present', 'future', 'adverb', 'preps','conj', 'quant', 'number', 'time', 'Sixltr',\n",
    "                    'Period', 'Colon', 'SemiC', 'QMark', 'Dash', 'Quote', 'Apostro', 'Parenth', 'OtherP',\n",
    "                    'negate', 'swear', 'posemo','negemo', 'assent', 'nonfl', 'filler', 'Exclam', 'insight',\n",
    "                    'cause', 'discrep', 'tentat', 'certain', 'inhib', 'incl', 'excl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prepare_mumsnet_data():\n",
    "        mumsnet_df = read_csv(dbdir+Mumsnet_DB, word_limit, min_WC)\n",
    "        mumsnet_between_set, mumsnet_within_set = split_between_within_sets(mumsnet_df)\n",
    "        mumsnet_train = get_train_set(mumsnet_between_set, batch_size, verbose=False)\n",
    "        mumsnet_test = get_test_set(mumsnet_within_set, batch_size, verbose=False)\n",
    "        \n",
    "        \n",
    "        source_train = mumsnet_train.reset_index(drop=True)\n",
    "        source_test = mumsnet_test.reset_index(drop=True)\n",
    "        \n",
    "        source_train_X = source_train[ALL_STYLISTIC_FEATURES].astype(float)\n",
    "        source_train_y = source_train['forum_id']\n",
    "        \n",
    "        source_test_X = source_test[ALL_STYLISTIC_FEATURES].astype(float)\n",
    "        source_test_y = source_test['forum_id']\n",
    "        \n",
    "        return source_train_X, source_test_X, source_train_y, source_test_y\n",
    "        \n",
    "\n",
    "def prepare_reddit_data():\n",
    "        reddit_df = read_csv(dbdir+Reddit_DB, word_limit, min_WC)\n",
    "        reddit_between_set, reddit_within_set = split_between_within_sets(reddit_df)\n",
    "        reddit_train = get_train_set(reddit_between_set, batch_size, verbose=False)\n",
    "        reddit_test = get_test_set(reddit_within_set, batch_size, verbose=False)\n",
    "        \n",
    "        target_train = reddit_train.reset_index(drop=True)\n",
    "        target_test = reddit_test.reset_index(drop=True)\n",
    "        \n",
    "        target_train_X = target_train[ALL_STYLISTIC_FEATURES].astype(float)\n",
    "        target_train_y = target_train['forum_id']\n",
    "        \n",
    "        target_test_X = target_test[ALL_STYLISTIC_FEATURES].astype(float)\n",
    "        target_test_y = target_test['forum_id']\n",
    "        \n",
    "        return target_train_X, target_test_X, target_train_y, target_test_y\n",
    "    \n",
    "    \n",
    "\n",
    "def prepare_experimental_data(test_condition='within', test_topic='hm'):\n",
    "    exp_within = pd.read_csv(dbdir + Exp_DB_within)\n",
    "    exp_between = pd.read_csv(dbdir + Exp_DB_between)\n",
    "\n",
    "    exp_data = pd.concat([exp_within, exp_between], axis=0)\n",
    "    \n",
    "    target_train_X = exp_data[ALL_STYLISTIC_FEATURES].astype(float)\n",
    "    target_train_y = exp_data['condition']\n",
    "    \n",
    "    if test_condition=='within':\n",
    "        test_df = exp_within.loc[exp_within.topic==test_topic]\n",
    "        target_test_X = test_df[ALL_STYLISTIC_FEATURES].astype(float)\n",
    "        target_test_y = test_df['condition']\n",
    "    elif test_condition=='between':\n",
    "        test_df = exp_between.loc[exp_between.topic==test_topic]\n",
    "        target_test_X = test_df[ALL_STYLISTIC_FEATURES].astype(float)\n",
    "        target_test_y = test_df['condition']\n",
    "        \n",
    "    \n",
    "    return target_train_X, target_test_X, target_train_y, target_test_y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The source data is Mumsnet dataset, and reddit dataset and experimental dataset are the target data. For reddit dataset, train data are chosen from the between samples, and test data are the within samples. For the esperimentad data, train data includes both the within and between samples, and test data can be chosen by determining two parameters of test_condition {'within', 'between'} and test_topic {'cc', 'ow', 'hm'}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading source and target data...\n",
      "\n",
      " source data size: 100000 \n",
      " traget_train_size: 456 \n",
      " traget_test_size: 110\n",
      "\n",
      " training a model on original source data ...\n",
      "model is trained\n",
      "\n",
      " test the model on original target test data ...\n",
      "\n",
      "\n",
      " accuracy: 0.6454545454545455 AUC: 0.6431924882629108\n",
      "tn: 7 fp: 32 fn: 7 tp: 64\n",
      "false positive error rate: 0.8205128205128205\n",
      "false negative error rate: 0.09859154929577464\n",
      "\n",
      " training a model on transformed source data ...\n",
      "model is trained\n",
      "\n",
      " test the model on transformed target test data: \n",
      "\n",
      "\n",
      " accuracy: 0.6454545454545455 AUC: 0.6710003611412062\n",
      "tn: 28 fp: 11 fn: 28 tp: 43\n",
      "false positive error rate: 0.28205128205128205\n",
      "false negative error rate: 0.39436619718309857\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size = 50000\n",
    "word_limit = True\n",
    "min_WC = 25\n",
    "features = ALL_STYLISTIC_FEATURES   \n",
    "dbdir = DBDIR\n",
    "\n",
    "\n",
    "print('loading source and target data...')\n",
    "source_train_X, source_test_X, source_train_y, source_test_y = prepare_mumsnet_data()\n",
    "# choose the target data - reddit data or experimental data\n",
    "#target_train_X, target_test_X, target_train_y, target_test_y = prepare_reddit_data()\n",
    "target_train_X, target_test_X, target_train_y, target_test_y = prepare_experimental_data(test_condition='between', \n",
    "                                                                                         test_topic='hm')\n",
    "\n",
    "print('\\n source data size:', source_train_X.shape[0], '\\n traget_train_size:', target_train_X.shape[0], \n",
    "     '\\n traget_test_size:', target_test_X.shape[0])\n",
    "\n",
    "\n",
    "# train a model on source data and test it on target (test) data before applying the DA\n",
    "print('\\n training a model on original source data ...')\n",
    "clf = train(source_train_X, source_train_y)\n",
    "\n",
    "print('\\n test the model on original target test data ...\\n')\n",
    "singletest(clf, target_test_X.copy(deep=True), target_test_y, verbose=True)\n",
    "\n",
    "\n",
    "# calculating the upper bound of dimensionality\n",
    "d_max = get_max_dimension(source_train_X[ALL_STYLISTIC_FEATURES].copy(deep=True), \n",
    "                          target_train_X[ALL_STYLISTIC_FEATURES].copy(deep=True), source_train_X.shape[1])\n",
    "\n",
    "# calculate the optimum dimensionality\n",
    "d_optimum = get_optimum_dimension(d_max, source_train_X[ALL_STYLISTIC_FEATURES].copy(deep=True), source_train_y, \n",
    "                                  target_train_X[ALL_STYLISTIC_FEATURES].copy(deep=True), verbose=True)\n",
    "\n",
    "\n",
    "# create the shared subspace according to the optimum number of dimensions\n",
    "X_a, X_t, scaler_a, scaler_t = get_shared_subspace(source_train_X.copy(deep=True),target_train_X.copy(deep=True), \n",
    "                                                   d_optimum)\n",
    "\n",
    "# transform the source and target test data\n",
    "X_1 = scaler_a.transform(source_train_X)\n",
    "X_2 = scaler_t.transform(target_test_X)\n",
    "\n",
    "S_a = np.matrix(X_1) * np.matrix(X_a)\n",
    "T_t = np.matrix(X_2) * np.matrix(X_t)\n",
    "\n",
    "y_s, y_t = source_train_y, target_test_y\n",
    "\n",
    "\n",
    "# train a model using transformed source train data\n",
    "print('\\n training a model on transformed source data ...')\n",
    "clf = train(pd.DataFrame(S_a), y_s)\n",
    "\n",
    "print('\\n test the model on transformed target test data: \\n')\n",
    "# test the model on transformed target test data\n",
    "singletest(clf, pd.DataFrame(T_t), y_t, verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
